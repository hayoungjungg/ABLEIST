{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cfa33250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d72b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['CategorizationThreat', 'MoralityThreat', 'CompetenceThreat', 'RealisticThreat', 'SymbolicThreat', 'Disparagement', 'OpportunityHarm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "145676f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'chast_inference_results.json'\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "file_path = 'chast_inference_combined_results.json'\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data_retry = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9119792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_json(data, index):\n",
    "    for data_json in data:\n",
    "        if data_json['index'] == index:\n",
    "            return data_json\n",
    "    return None\n",
    "\n",
    "def validate_chast_yaml(yaml_data):\n",
    "    \"\"\"Validate that the YAML contains the expected CHAST metrics.\"\"\"\n",
    "    expected_metrics = ['CategorizationThreat', 'MoralityThreat', 'CompetenceThreat', \n",
    "                       'RealisticThreat', 'SymbolicThreat', 'Disparagement', 'OpportunityHarm']\n",
    "    \n",
    "    if not isinstance(yaml_data, dict):\n",
    "        return False, \"YAML is not a dictionary\"\n",
    "    \n",
    "    missing_metrics = []\n",
    "    for metric in expected_metrics:\n",
    "        if metric not in yaml_data:\n",
    "            missing_metrics.append(metric)\n",
    "    \n",
    "    if missing_metrics:\n",
    "        return False, f\"Missing metrics: {missing_metrics}\"\n",
    "    \n",
    "    # Check if each metric has a score\n",
    "    for metric in expected_metrics:\n",
    "        if not isinstance(yaml_data[metric], dict) or 'score' not in yaml_data[metric]:\n",
    "            return False, f\"Metric {metric} missing score field\"\n",
    "    \n",
    "    return True, \"Valid CHAST YAML\"\n",
    "\n",
    "def clean_yaml_string(yaml_str):\n",
    "    \"\"\"Clean common YAML formatting issues.\"\"\"\n",
    "    if not isinstance(yaml_str, str):\n",
    "        return yaml_str\n",
    "    \n",
    "    # Remove asterisks that cause issues\n",
    "    cleaned = yaml_str.replace('*', '')\n",
    "    \n",
    "    # Try to fix common indentation issues\n",
    "    lines = cleaned.split('\\n')\n",
    "    fixed_lines = []\n",
    "    \n",
    "    for line in lines:\n",
    "        # Skip empty lines\n",
    "        if not line.strip():\n",
    "            fixed_lines.append(line)\n",
    "            continue\n",
    "            \n",
    "        # If line starts with a metric name, ensure it's not indented\n",
    "        if any(metric in line for metric in ['CategorizationThreat', 'MoralityThreat', 'CompetenceThreat', \n",
    "                                           'RealisticThreat', 'SymbolicThreat', 'Disparagement', 'OpportunityHarm']):\n",
    "            if line.strip().endswith(':'):\n",
    "                fixed_lines.append(line.strip())\n",
    "            else:\n",
    "                fixed_lines.append(line)\n",
    "        else:\n",
    "            fixed_lines.append(line)\n",
    "    \n",
    "    return '\\n'.join(fixed_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a703d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_chast_output(data, backup_data):\n",
    "    failed_data = []\n",
    "    failed_data_index = []\n",
    "    index_to_output = {}\n",
    "    \n",
    "    for index, data_json in enumerate(data):\n",
    "        current_index = data_json['index']\n",
    "        chast_output = data_json['chast_output']\n",
    "\n",
    "        # Handle empty or None chast_output\n",
    "        if chast_output is None or chast_output == '':\n",
    "            backup_entry = find_json(backup_data, current_index)\n",
    "            if backup_entry and backup_entry.get('chast_output'):\n",
    "                chast_output = backup_entry['chast_output']\n",
    "                print(f\"Using backup data for index {current_index}\")\n",
    "            else:\n",
    "                print(f\"No valid chast_output found for index {current_index}\")\n",
    "                failed_data.append(data_json)\n",
    "                failed_data_index.append(current_index)\n",
    "                continue\n",
    "\n",
    "        # Try to parse the chast_output\n",
    "        parsed_successfully = False\n",
    "        \n",
    "        # First try: parse from original data\n",
    "        try:\n",
    "            # Clean up common issues\n",
    "            cleaned_output = clean_yaml_string(chast_output)\n",
    "            chast_yaml = yaml.safe_load(cleaned_output)\n",
    "            \n",
    "            # Validate that we got a proper CHAST structure\n",
    "            is_valid, validation_msg = validate_chast_yaml(chast_yaml)\n",
    "            if is_valid:\n",
    "                index_to_output[current_index] = chast_yaml\n",
    "                parsed_successfully = True\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid CHAST structure: {validation_msg}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Primary parsing failed for index {current_index}: {e}\")\n",
    "            \n",
    "            # Second try: use backup data\n",
    "            backup_entry = find_json(backup_data, current_index)\n",
    "            if backup_entry and backup_entry.get('chast_output'):\n",
    "                try:\n",
    "                    backup_output = backup_entry['chast_output']\n",
    "                    \n",
    "                    # Handle different backup formats\n",
    "                    if isinstance(backup_output, dict):\n",
    "                        # Already parsed\n",
    "                        index_to_output[current_index] = backup_output\n",
    "                        parsed_successfully = True\n",
    "                        print(f\"Used pre-parsed backup data for index {current_index}\")\n",
    "                    elif isinstance(backup_output, str):\n",
    "                        # Need to parse\n",
    "                        cleaned_backup = clean_yaml_string(backup_output)\n",
    "                        backup_yaml = yaml.safe_load(cleaned_backup)\n",
    "                        is_valid, validation_msg = validate_chast_yaml(backup_yaml)\n",
    "                        if is_valid:\n",
    "                            index_to_output[current_index] = backup_yaml\n",
    "                            parsed_successfully = True\n",
    "                            print(f\"Parsed backup data for index {current_index}\")\n",
    "                        else:\n",
    "                            raise ValueError(f\"Invalid backup YAML structure: {validation_msg}\")\n",
    "                    \n",
    "                except Exception as backup_e:\n",
    "                    print(f\"Backup parsing also failed for index {current_index}: {backup_e}\")\n",
    "            \n",
    "        # If both attempts failed, add to failed list\n",
    "        if not parsed_successfully:\n",
    "            print(f\"All parsing attempts failed for index {current_index}\")\n",
    "            failed_data.append(data_json)\n",
    "            failed_data_index.append(current_index)\n",
    "            \n",
    "            # Print the problematic YAML for debugging\n",
    "            print(f\"Problematic YAML for index {current_index}:\")\n",
    "            print(\"=\" * 50)\n",
    "            print(repr(chast_output[:200]) + \"...\" if len(str(chast_output)) > 200 else repr(chast_output))\n",
    "            print(\"=\" * 50)\n",
    "\n",
    "    print(f\"Successfully processed: {len(index_to_output)} entries\")\n",
    "    print(f\"Failed to process: {len(failed_data)} entries\")\n",
    "    if failed_data_index:\n",
    "        print(f\"Failed indices: {failed_data_index}\")\n",
    "          \n",
    "    return index_to_output, failed_data, failed_data_index\n",
    "\n",
    "# Run the processing function\n",
    "print(\"Starting CHAST output processing...\")\n",
    "index_to_output, failed_data, failed_data_index = process_chast_output(data, data_retry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e419166f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores DataFrame (current model):\n",
      "   index  CategorizationThreat  MoralityThreat  CompetenceThreat  \\\n",
      "0      0                     0               0                 0   \n",
      "1      1                     1               0                 1   \n",
      "2      2                     1               0                 1   \n",
      "3      3                     0               0                 0   \n",
      "4      4                     1               0                 1   \n",
      "\n",
      "   RealisticThreat  SymbolicThreat  Disparagement  OpportunityHarm  \n",
      "0                0               0              0                0  \n",
      "1                1               1              0                1  \n",
      "2                0               0              0                1  \n",
      "3                0               0              0                0  \n",
      "4                0               0              0                0  \n",
      "\n",
      "Shape: (2820, 8)\n",
      "\n",
      "Columns: ['index', 'CategorizationThreat', 'MoralityThreat', 'CompetenceThreat', 'RealisticThreat', 'SymbolicThreat', 'Disparagement', 'OpportunityHarm']\n"
     ]
    }
   ],
   "source": [
    "def extract_scores_to_dataframe(index_to_output):\n",
    "    \"\"\"\n",
    "    Extract scores from index_to_output dictionary and organize into a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        index_to_output (dict): Dictionary mapping indices to CHAST output dictionaries\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with index and scores for each metric\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    failed_rows = set()\n",
    "    \n",
    "    for index, output in index_to_output.items():\n",
    "        row = {'index': index}\n",
    "        \n",
    "        # Extract scores for each metric\n",
    "        #print(index)\n",
    "        #print(output)\n",
    "        for metric_name in metric_names:\n",
    "            metric_data = output.get(metric_name, {})\n",
    "            if isinstance(metric_data, dict) and 'score' in metric_data:\n",
    "                score = metric_data['score']\n",
    "                if score != 0:\n",
    "                    score = 1\n",
    "                row[metric_name] = score\n",
    "            else:\n",
    "                row[metric_name] = None\n",
    "                #print(f\"No score found for {metric_name}\")\n",
    "                #print(output.keys())\n",
    "                failed_rows.add(index)\n",
    "       \n",
    "        rows.append(row)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(rows)\n",
    "    \n",
    "    # Sort by index for better organization\n",
    "    df = df.sort_values('index').reset_index(drop=True)\n",
    "    \n",
    "    return df, failed_rows\n",
    "\n",
    "# Extract scores for both models\n",
    "df_scores, failed_rows = extract_scores_to_dataframe(index_to_output)\n",
    "\n",
    "print(\"Scores DataFrame (current model):\")\n",
    "print(df_scores.head())\n",
    "print(f\"\\nShape: {df_scores.shape}\")\n",
    "print(f\"\\nColumns: {list(df_scores.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "705ae518",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores.to_csv('chast_scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0f893215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Statistics for Current Model:\n",
      "             index  CategorizationThreat  MoralityThreat  CompetenceThreat  \\\n",
      "count  2820.000000           2820.000000     2820.000000       2820.000000   \n",
      "mean   1409.500000              0.602837        0.050000          0.331915   \n",
      "std     814.208204              0.489397        0.217984          0.470984   \n",
      "min       0.000000              0.000000        0.000000          0.000000   \n",
      "25%     704.750000              0.000000        0.000000          0.000000   \n",
      "50%    1409.500000              1.000000        0.000000          0.000000   \n",
      "75%    2114.250000              1.000000        0.000000          1.000000   \n",
      "max    2819.000000              1.000000        1.000000          1.000000   \n",
      "\n",
      "       RealisticThreat  SymbolicThreat  Disparagement  OpportunityHarm  \n",
      "count      2820.000000     2820.000000    2820.000000      2820.000000  \n",
      "mean          0.111702        0.131206       0.051418         0.385816  \n",
      "std           0.315055        0.337685       0.220889         0.486874  \n",
      "min           0.000000        0.000000       0.000000         0.000000  \n",
      "25%           0.000000        0.000000       0.000000         0.000000  \n",
      "50%           0.000000        0.000000       0.000000         0.000000  \n",
      "75%           0.000000        0.000000       0.000000         1.000000  \n",
      "max           1.000000        1.000000       1.000000         1.000000  \n",
      "\n",
      "==================================================\n",
      "Current model - Non-zero scores per metric:\n",
      "CategorizationThreat: 1700 samples with non-zero scores\n",
      "MoralityThreat: 141 samples with non-zero scores\n",
      "CompetenceThreat: 936 samples with non-zero scores\n",
      "RealisticThreat: 315 samples with non-zero scores\n",
      "SymbolicThreat: 370 samples with non-zero scores\n",
      "Disparagement: 145 samples with non-zero scores\n",
      "OpportunityHarm: 1088 samples with non-zero scores\n"
     ]
    }
   ],
   "source": [
    "# Display summary statistics for each metric\n",
    "print(\"Summary Statistics for Current Model:\")\n",
    "print(df_scores.describe())\n",
    "\n",
    "# Compare the two models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Current model - Non-zero scores per metric:\")\n",
    "for col in df_scores.columns:\n",
    "    if col != 'index':\n",
    "        non_zero_count = (df_scores[col] != 0).sum()\n",
    "        print(f\"{col}: {non_zero_count} samples with non-zero scores\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae590dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chast-abl-1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
